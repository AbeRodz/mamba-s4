{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssm import SSM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = SSM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_t = torch.randn((2, 512, 64), device=\"cuda\", dtype=torch.float32)\n",
    "A_bar = torch.randn((2, 512, 64, 2), device=\"cuda\", dtype=torch.float32)\n",
    "B_bar_x = torch.randn_like(A_bar)\n",
    "C = torch.randn((2, 64, 2), device=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 64]) torch.Size([2, 512, 64, 2]) torch.Size([2, 512, 64, 2]) torch.Size([2, 64, 2])\n"
     ]
    }
   ],
   "source": [
    "print(h_t.shape,A_bar.shape,B_bar_x.shape,C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b ,c = space.random_SSM(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2]) torch.Size([2, 1]) torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(a.shape,b.shape,c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3.1297, 1.9813],\n",
       "         [1.9116, 2.6228]]),\n",
       " tensor([[1.8986],\n",
       "         [1.6967]]),\n",
       " tensor([[0.5462, 0.3724]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space.discretize(a,b,c,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.0200, 2.0326],\n",
       "         [1.9824, 1.6847]]),\n",
       " tensor([[2.2403],\n",
       "         [0.1713]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space.discretize_zoh(a,b,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.ones([3])\n",
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_log(float_input, eps=1e-6):\n",
    "        eps = float_input.new_tensor(eps)\n",
    "        real = float_input.abs().maximum(eps).log()\n",
    "        imag = (float_input < 0).to(float_input.dtype) * torch.pi\n",
    "        return torch.complex(real, imag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TODO READ THE S4 code the issue is on the input dimensionality, seems that einsum might be the answer to this issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure and Dimensions.\n",
    "Finally, we note that structured SSMs are so named because computing them efficiently also requires imposing structure on the A matrix. The most popular form of structure is diagonal which we also use.\n",
    "\n",
    "In this case, the A ∈ ℝN×N, B ∈ ℝN×1, C ∈ ℝ1×N matrices can all be represented by N numbers. \n",
    "\n",
    "To operate over an input sequence x of batch size B and length L with D channels, the SSM is applied independently to each\n",
    "channel.\n",
    "\n",
    "Note that in this case, the total hidden state has dimension DN per input, and computing it over the\n",
    "sequence length requires O(BLDN) time and memory; this is the root of the fundamental efficiency bottleneck\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dimesnions \n",
    " - Input(x) and output(y) are of size BLD\n",
    " - A : (DN)\n",
    " - B : (BLN) \n",
    " - C : (BLN)\n",
    " - Delta : (BLD)\n",
    " - A_BAR, B_BAR : (BLDN) discretize with (Delta,AB)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_scan( h_t: torch.Tensor ,A_bar : torch.Tensor, B_bar: torch.Tensor, C: torch.Tensor):\n",
    "        ## SSM COMPATIBLE\n",
    "        # h_t = A_bar * h_t-1 + B_bar*x_t\n",
    "        #y_t =C*h_t\n",
    "\n",
    "        ## heinsen\n",
    "        # x_t = a_t*x_t-1 + b_t\n",
    "\n",
    "        # equivalent notation\n",
    "\n",
    "        # h_t = A_bar_t * h_t-1 + B_bar*x_t\n",
    "        # where x_t is the input sequence hence:\n",
    "        # cat(B_bar,  x_t) = b_t' or b_t prime.\n",
    "        # finally:\n",
    "        # h_t = A_bar_t * h_t-1 + b_t'\n",
    "        \"\"\"\n",
    "        In summary we need to calculate a state space equation (S4) using the heinsen method.\n",
    "\n",
    "        \"\"\"\n",
    "        #b_t_prime =  torch.cat([h_t[..., None], B_bar], dim=-1)\n",
    "        #b_t_prime = torch.matmul(B_bar,h_t)\n",
    "        #b_t_prime = B_bar @ h_t\n",
    "        b_t_prime = torch.einsum(\"bld,bln->bldn\", h_t,B_bar)\n",
    "        log_b_t = complex_log(b_t_prime)\n",
    "        log_a_t = complex_log(A_bar)\n",
    "        a_star = F.pad(torch.cumsum(log_a_t, dim = -1),(1,0))\n",
    "        #a_star = torch.cumsum(log_a_t, dim = -1)\n",
    "        print(log_b_t.shape)\n",
    "        print(a_star.shape)\n",
    "        log_h0_plus_b_star = torch.logcumsumexp(log_b_t - a_star, dim = -1)\n",
    "\n",
    "        log_h = (a_star + log_h0_plus_b_star)[...,1:]\n",
    "        \n",
    "        h = torch.exp(log_h).real\n",
    "        print(h.shape)\n",
    "        y_t = torch.matmul(C, h).squeeze(-1)\n",
    "        #y_t = torch.einsum(\"bnt,bdnt->bdt\", C, h) \n",
    "        return h, y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_scan(u, dt, A, B, C):\n",
    "        dA = torch.einsum('bld,dn->bldn', dt, A)\n",
    "        dB_u = torch.einsum('bld,bld,bln->bldn', dt, u, B)\n",
    "        \n",
    "        dA_cumsum = F.pad(dA[:, 1:], (0, 0, 0, 0, 0, 1)).flip(1).cumsum(1).exp().flip(1)\n",
    "        x = dB_u * dA_cumsum\n",
    "        x = x.cumsum(1) / (dA_cumsum + 1e-12)\n",
    "        y = torch.einsum('bldn,bln->bld', x, C)\n",
    "    \n",
    "        return y + u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_in_parallel(coeffs, values):\n",
    "    log_coeffs = complex_log(coeffs)\n",
    "    log_values = complex_log(values)\n",
    "    a_star = F.pad(torch.cumsum(log_coeffs, dim=-1), (1, 0))              # eq (2) in paper\n",
    "    print(log_values.shape)\n",
    "    print(a_star.shape)\n",
    "    log_x0_plus_b_star = torch.logcumsumexp(log_values - a_star, dim=-1)  # eq (7) in paper\n",
    "    log_x = a_star + log_x0_plus_b_star                                   # eq (1) in paper\n",
    "    return torch.exp(log_x).real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = torch.Tensor([[1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2]) torch.Size([2, 1]) torch.Size([1, 2]) torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(a.shape,b.shape,c.shape,u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (2) for operand 0 and no ellipsis was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mparallel_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 23\u001b[0m, in \u001b[0;36mparallel_scan\u001b[1;34m(h_t, A_bar, B_bar, C)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mIn summary we need to calculate a state space equation (S4) using the heinsen method.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#b_t_prime =  torch.cat([h_t[..., None], B_bar], dim=-1)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#b_t_prime = torch.matmul(B_bar,h_t)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#b_t_prime = B_bar @ h_t\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m b_t_prime \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbld,bln->bldn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43mB_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m log_b_t \u001b[38;5;241m=\u001b[39m complex_log(b_t_prime)\n\u001b[0;32m     25\u001b[0m log_a_t \u001b[38;5;241m=\u001b[39m complex_log(A_bar)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\python_stuff\\venv\\Lib\\site-packages\\torch\\functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (2) for operand 0 and no ellipsis was given"
     ]
    }
   ],
   "source": [
    "parallel_scan(u,a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n",
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.6294, 1.0719, 1.3897],\n",
       "        [0.6046, 1.0184, 1.1358]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_in_parallel(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m h , y \u001b[38;5;241m=\u001b[39m \u001b[43mparallel_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 30\u001b[0m, in \u001b[0;36mparallel_scan\u001b[1;34m(h_t, A_bar, B_bar, C)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(log_b_t\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(a_star\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 30\u001b[0m log_h0_plus_b_star \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogcumsumexp(\u001b[43mlog_b_t\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ma_star\u001b[49m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     32\u001b[0m log_h \u001b[38;5;241m=\u001b[39m (a_star \u001b[38;5;241m+\u001b[39m log_h0_plus_b_star)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     34\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(log_h)\u001b[38;5;241m.\u001b[39mreal\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "h , y = parallel_scan(u, a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000e-06],\n",
       "        [1.0000e+00]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0000e-06])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_mass(k, b, m):\n",
    "    A = np.array([[0, 1], [-k / m, -b / m]])\n",
    "    B = np.array([[0], [1.0 / m]])\n",
    "    C = np.array([[1.0, 0]])\n",
    "    return torch.Tensor(A), torch.Tensor(B), torch.Tensor(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = example_mass(1,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:torch.Size([2, 2])\n",
      "1:torch.Size([2, 1])\n",
      "2:torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print_shapes([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_force(t):\n",
    "    x = np.sin(10 * t)\n",
    "    return torch.Tensor(x * (x > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = example_force(np.arange(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.Tensor([1,1])\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.],\n",
       "        [-1., -5.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.],\n",
       "        [-1., -5.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a * u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((2, 2))\n",
    "y = torch.rand((1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3428, 0.8222]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7660],\n",
       "        [0.8331]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum(\"ij,kj->ik\",X,y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (1) for operand 0 and no ellipsis was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m h , y \u001b[38;5;241m=\u001b[39m \u001b[43mparallel_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 23\u001b[0m, in \u001b[0;36mparallel_scan\u001b[1;34m(h_t, A_bar, B_bar, C)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mIn summary we need to calculate a state space equation (S4) using the heinsen method.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#b_t_prime =  torch.cat([h_t[..., None], B_bar], dim=-1)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#b_t_prime = torch.matmul(B_bar,h_t)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#b_t_prime = B_bar @ h_t\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m b_t_prime \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbld,bln->bldn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43mB_bar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m log_b_t \u001b[38;5;241m=\u001b[39m complex_log(b_t_prime)\n\u001b[0;32m     25\u001b[0m log_a_t \u001b[38;5;241m=\u001b[39m complex_log(A_bar)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\python_stuff\\venv\\Lib\\site-packages\\torch\\functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (1) for operand 0 and no ellipsis was given"
     ]
    }
   ],
   "source": [
    "h , y = parallel_scan(u, a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.737088e-05, 4.968198e-05], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e-06, 2.0000e-06],\n",
       "        [6.6002e-15, 1.0000e+00]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0.,   1.],\n",
       "         [-40.,  -5.]]),\n",
       " tensor([[0.],\n",
       "         [1.]]),\n",
       " tensor([[1., 0.]]))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_mass(40,5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5646, 0.6442, 0.7174,\n",
       "        0.7833, 0.8415, 0.8912, 0.9320, 0.9636, 0.9854, 0.9975, 0.9996, 0.9917,\n",
       "        0.9738, 0.9463, 0.9093, 0.8632, 0.8085, 0.7457, 0.6755, 0.5985, 0.5155,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5784, 0.6570, 0.7290,\n",
       "        0.7937, 0.8504, 0.8987, 0.9380, 0.9679, 0.9882, 0.9985, 0.9989, 0.9894,\n",
       "        0.9699, 0.9407, 0.9022, 0.8546, 0.7985, 0.7344, 0.6630, 0.5849, 0.5010,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_scan(\n",
    "    h_t: torch.FloatTensor,\n",
    "    A_bar: torch.FloatTensor,\n",
    "    B_bar_x: torch.FloatTensor,\n",
    "    C: torch.FloatTensor,\n",
    "    eps: float = 1e-8,\n",
    ") -> torch.FloatTensor:\n",
    "    \"\"\"\n",
    "    Parallel computation of the selective scan based on the algorithm described in\n",
    "    \"Efficient Parallelization of a Ubiquitous Sequential Computation\". This method\n",
    "    transforms the sequential operation `x_t = a_t * x_{t−1} + b_t` into a form\n",
    "    suitable for parallel processing. It computes `log(x_t) = a_star_t + log(x_0 + b_star_t)`\n",
    "    using parallel cumulative sums, where `a_star_t = cumsum(log(a_t))` and\n",
    "    `b_star_t = logcumsumexp(log(b_t - a_star_t))`. This approach significantly\n",
    "    enhances computational efficiency for large-scale tensor operations. In this method, we use the\n",
    "    above formulation to compute `h_t = a_t * h_{t−1} + b_t` and `y_t = C_t h_t` in parallel.\n",
    "\n",
    "    Reference:\n",
    "    [Efficient Parallelization of a Ubiquitous Sequential Computation](https://arxiv.org/pdf/2311.06281.pdf)\n",
    "    \"\"\"\n",
    "    # Combine B_bar_x and h_t into a single tensor\n",
    "    B_bar_x = torch.cat([h_t[..., None], B_bar_x], dim=-1)  # [b x d x n x t + 1]\n",
    "\n",
    "    # Compute the log of the parameters in complex space as log of negative numbers is complex\n",
    "    log_A_bar = (\n",
    "        A_bar.masked_fill(A_bar == 0, eps).to(torch.complex64)\n",
    "    ).log()  # log a_t (eq. 2 - Efficient Parallelization)\n",
    "    log_B_bar_x = (\n",
    "        B_bar_x.masked_fill(B_bar_x == 0, eps).to(torch.complex64)\n",
    "    ).log()  # log b_t (eq. 2 - Efficient Parallelization)\n",
    "\n",
    "    # Compute the cumulative sum of log_A_bar along the time dimension\n",
    "    A_bar_star = F.pad(\n",
    "        torch.cumsum(log_A_bar, dim=-1), (1, 0)\n",
    "    )  # a_star_t (eq. 2 - Efficient Parallelization) [b x d x n x t + 1]\n",
    "\n",
    "    # Compute the log of the cumulative sum of log_B_bar_x along the time dimension\n",
    "    B_bar_x_star = torch.logcumsumexp(\n",
    "        log_B_bar_x - A_bar_star, dim=-1\n",
    "    )  # b_star_t (eq. 2 - Efficient Parallelization) [b x d x n x t + 1]\n",
    "\n",
    "    log_h_ts = (A_bar_star + B_bar_x_star)[\n",
    "        ..., 1:\n",
    "    ]  # log x_t (eq. 1 - Efficient Parallelization)\n",
    "    h_ts = torch.exp(\n",
    "        log_h_ts\n",
    "    ).real  # x_t (eq. 3 - Efficient Parallelization) h_t = ̄A_t h_t + ̄Bx_t (eq. 2a - Mamba)\n",
    "    print(h_ts.shape)\n",
    "    return torch.einsum(\"bnt,bdnt->bdt\", C, h_ts)  # y_t = C_t h_t  (eq. 2b - Mamba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_SSM(Ab, Bb, Cb, u, x0):\n",
    "    def step(x_k_1, u_k):\n",
    "        x_k = Ab @ x_k_1 + Bb @ u_k\n",
    "        y_k = Cb @ x_k\n",
    "        return x_k, y_k\n",
    "\n",
    "    return jax.lax.scan(step, x0, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dimesnions  S4\n",
    " - Input(x) and output(y) are of size BLD\n",
    " - A : (DN)\n",
    " - B : (DN) \n",
    " - C : (DN)\n",
    " - Delta : (D)\n",
    " - A_BAR, B_BAR : (DN) discretize with (Delta,AB)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSM_S4(dt: torch.Tensor, h_t: torch.Tensor ,A : torch.Tensor, B: torch.Tensor, C: torch.Tensor):\n",
    "\n",
    "    dA = torch.exp(torch.einsum(\"d,dn-> dn\", dt, A))\n",
    "    print(\"dA:\", dA.shape)\n",
    "    dB_u = torch.einsum(\"d,dn,bld->bldn\",dt,B,h_t)\n",
    "    log_da = complex_log(dA)\n",
    "    print(\"dB_u:\", dB_u.shape)\n",
    "\n",
    "    log_dB_u = complex_log(dB_u)\n",
    "    print(\"log_da:\",log_da.shape)\n",
    "    print(\"log_dB_u:\",log_dB_u.shape)\n",
    "    a_star = F.pad(torch.cumsum(log_da,dim =-1),(0,0,0,1))\n",
    "    log_x0_plus_b_star = torch.logcumsumexp(log_dB_u - a_star, dim=-1)  \n",
    "    log_x = a_star + log_x0_plus_b_star                                   # eq (1) in paper\n",
    "    x = torch.exp(log_x).real\t\n",
    "    y = torch.einsum('blnn,dn->bld', x,C)\n",
    "    return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSM_S4(dt: torch.Tensor, h_t: torch.Tensor ,A : torch.Tensor, B: torch.Tensor, C: torch.Tensor):\n",
    "    # with 2 dim\n",
    "    print(h_t.shape)\n",
    "    dA = torch.exp(torch.einsum(\"d,dn-> dn\", dt, A))\n",
    "    print(\"dA:\", dA.shape)\n",
    "    if h_t.ndim >2:\n",
    "        dB_u = torch.einsum(\"d,dn,bld->bldn\",dt,B,h_t)\n",
    "    else:\n",
    " \n",
    "        dB_u = torch.einsum(\"d,dn,bd->bdn\",dt,B,h_t)\n",
    "    log_da = complex_log(dA)\n",
    "    print(\"dB_u:\", dB_u.shape)\n",
    "\n",
    "    log_dB_u = complex_log(dB_u)\n",
    "    print(\"log_da:\",log_da.shape)\n",
    "    print(\"log_dB_u:\",log_dB_u.shape)\n",
    "    if h_t.ndim >2:\n",
    "        a_star = F.pad(torch.cumsum(log_da,dim =-1),(0,0,0,1))  \n",
    "    else:\n",
    "        a_star = F.pad(torch.cumsum(log_da,dim =-1),(0,1))  \n",
    "    print(a_star.shape)\n",
    "    log_x0_plus_b_star = torch.logcumsumexp(log_dB_u - a_star, dim=-1)  \n",
    "    log_x = a_star + log_x0_plus_b_star                                   # eq (1) in paper\n",
    "    x = torch.exp(log_x).real\t\n",
    "    print(x.shape)\n",
    "    print(C.shape)\n",
    "\n",
    "    y = torch.einsum('bij,ki->bij', x,C)\n",
    "    print(y.shape)\n",
    "    return y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1\n",
    "n = 2\n",
    "b = 3\n",
    "l = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = torch.rand((d))\n",
    "A = torch.rand((d,n))\n",
    "B = torch.rand((d,n))\n",
    "C =  torch.rand((d,n))\n",
    "h_t =torch.rand(b,l,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shapes(tensors : list[torch.Tensor]):\n",
    "    for i,tensor in enumerate(tensors):\n",
    "        print(f\"{i}:{tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:torch.Size([1, 2])\n",
      "1:torch.Size([1, 2])\n",
      "2:torch.Size([1, 2])\n",
      "3:torch.Size([3, 4, 1])\n",
      "4:torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print_shapes([A,B,C,h_t, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA: torch.Size([1, 2])\n",
      "dB_u: torch.Size([3, 4, 1, 2])\n",
      "log_da: torch.Size([1, 2])\n",
      "log_dB_u: torch.Size([3, 4, 1, 2])\n",
      "tensor([[[0.0729],\n",
      "         [0.2119],\n",
      "         [0.1471],\n",
      "         [0.1033]],\n",
      "\n",
      "        [[0.0032],\n",
      "         [0.2574],\n",
      "         [0.1191],\n",
      "         [0.2909]],\n",
      "\n",
      "        [[0.0178],\n",
      "         [0.2618],\n",
      "         [0.3042],\n",
      "         [0.2435]]])\n"
     ]
    }
   ],
   "source": [
    "y = SSM_S4(dt,h_t,A,B,C )\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_SSM(A : torch.Tensor, B : torch.Tensor, C : torch.Tensor, u : torch.Tensor):\n",
    "\n",
    "    #L = u.shape[0]\n",
    "    #N = A.shape[0]\n",
    "    dt = torch.rand((1))\n",
    "    #a_b , b_b , c_b = space.discretize(A,B,C,1.0/L)\n",
    "    return SSM_S4(dt,u[:,np.newaxis], A,B,C)[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA: torch.Size([1, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (4) for operand 2 and no ellipsis was given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[265], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mrun_SSM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43mh_t\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[262], line 7\u001b[0m, in \u001b[0;36mrun_SSM\u001b[1;34m(A, B, C, u)\u001b[0m\n\u001b[0;32m      5\u001b[0m dt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#a_b , b_b , c_b = space.discretize(A,B,C,1.0/L)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSSM_S4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mu\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[1;32mIn[264], line 5\u001b[0m, in \u001b[0;36mSSM_S4\u001b[1;34m(dt, h_t, A, B, C)\u001b[0m\n\u001b[0;32m      3\u001b[0m dA \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md,dn-> dn\u001b[39m\u001b[38;5;124m\"\u001b[39m, dt, A))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdA:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dA\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 5\u001b[0m dB_u \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43md,dn,bld->bldn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43mh_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m log_da \u001b[38;5;241m=\u001b[39m complex_log(dA)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdB_u:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dB_u\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\python_stuff\\venv\\Lib\\site-packages\\torch\\functional.py:377\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    379\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (4) for operand 2 and no ellipsis was given"
     ]
    }
   ],
   "source": [
    "y = run_SSM(A,B,C,h_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4017762  0.5127762  0.0409007  0.15183365]\n"
     ]
    }
   ],
   "source": [
    "print(y.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:torch.Size([2, 2])\n",
      "1:torch.Size([2, 1])\n",
      "2:torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print_shapes(example_mass(k=40, b=5, m=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOLVE PENDING DIMENSIONALTIY ISSUES\n",
    "\n",
    "ON the current s4 scan implementation in only takes in consideration the fixed dimensions established on the paper, although in this case it should be able to handle dimensions such as the one defined on the example mass, which represents a real state space, need to implement this condition, and this way the code below should work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u[:, np.newaxis].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions seems to be ok although numerical there seems to be an issue, check the ssm function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n",
      "dA: torch.Size([2, 2])\n",
      "dB_u: torch.Size([100, 2, 1])\n",
      "log_da: torch.Size([2, 2])\n",
      "log_dB_u: torch.Size([100, 2, 1])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([100, 2, 3])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([100, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       9.9999988e-07, 2.1401838e-06, 2.8770498e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 9.9999988e-07, 2.1401838e-06,\n",
       "       2.8770498e-06, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SSM_S4(torch.rand((1)),u[:,np.newaxis], *ssm).reshape(-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n",
      "dA: torch.Size([2, 2])\n",
      "dB_u: torch.Size([100, 2, 1])\n",
      "log_da: torch.Size([2, 2])\n",
      "log_dB_u: torch.Size([100, 2, 1])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([100, 2, 3])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([100, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "y = run_SSM(*ssm, u).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.9999988e-07, 2.2816862e-06, 2.7802209e-06, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5646, 0.6442, 0.7174,\n",
      "        0.7833, 0.8415, 0.8912, 0.9320, 0.9636, 0.9854, 0.9975, 0.9996, 0.9917,\n",
      "        0.9738, 0.9463, 0.9093, 0.8632, 0.8085, 0.7457, 0.6755, 0.5985, 0.5155,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5784, 0.6570, 0.7290,\n",
      "        0.7937, 0.8504, 0.8987, 0.9380, 0.9679, 0.9882, 0.9985, 0.9989, 0.9894,\n",
      "        0.9699, 0.9407, 0.9022, 0.8546, 0.7985, 0.7344, 0.6630, 0.5849, 0.5010,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.0000])\n",
      "torch.Size([100, 1])\n",
      "dA: torch.Size([2, 2])\n",
      "dB_u: torch.Size([100, 2, 1])\n",
      "log_da: torch.Size([2, 2])\n",
      "log_dB_u: torch.Size([100, 2, 1])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([100, 2, 3])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([100, 2, 3])\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n",
      "[-0.04, 0.0, 0.04]\n",
      "[-0.03999900000011621, 9.999999e-07, 0.04000099999988379]\n",
      "[-0.039997325413005456, 2.674587e-06, 0.040002674586994545]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ssm = example_mass(k=40, b=5, m=1)\n",
    "\n",
    "# L samples of u(t).\n",
    "L = 100\n",
    "step = 1.0 / L\n",
    "ks = np.arange(L)\n",
    "u = example_force(ks * step)\n",
    "print(u)\n",
    "# Approximation of y(t).\n",
    "y = SSM_S4(torch.rand((1)),u[:,np.newaxis], *ssm).reshape(-1).numpy()\n",
    "# Plotting ---\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from celluloid import Camera\n",
    "\n",
    "seaborn.set_context(\"paper\")\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "camera = Camera(fig)\n",
    "ax1.set_title(\"Force $u_k$\")\n",
    "ax2.set_title(\"Position $y_k$\")\n",
    "#ax3.set_title(\"Object\")\n",
    "ax1.set_xticks([], [])\n",
    "ax2.set_xticks([], [])\n",
    "\n",
    "# Animate plot over time\n",
    "for k in range(0, L, 2):\n",
    "    print([y[k] - 0.04, y[k], y[k] + 0.04])\n",
    "    ax1.plot(ks[:k], u[:k], color=\"red\")\n",
    "    ax2.plot(ks[:k], y[:k], color=\"blue\")\n",
    "    ax3.boxplot(\n",
    "        [[y[k, 0] - 0.04, y[k, 0], y[k, 0] + 0.04]],\n",
    "        showcaps=False,\n",
    "        whis=False,\n",
    "        vert=False,\n",
    "        widths=10,\n",
    "    )\n",
    "    camera.snap()\n",
    "anim = camera.animate()\n",
    "anim.save(\"line.gif\", dpi=150, writer=\"imagemagick\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_ssm():\n",
    "    # SSM\n",
    "    ssm = example_mass(k=40, b=5, m=1)\n",
    "\n",
    "    # L samples of u(t).\n",
    "    L = 100\n",
    "    step = 1.0 / L\n",
    "    ks = np.arange(L)\n",
    "    u = example_force(ks * step)\n",
    "\n",
    "    # Approximation of y(t).\n",
    "    y = run_SSM(*ssm, u)\n",
    "    print\n",
    "    # Plotting ---\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn\n",
    "    from celluloid import Camera\n",
    "\n",
    "    seaborn.set_context(\"paper\")\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
    "    camera = Camera(fig)\n",
    "    ax1.set_title(\"Force $u_k$\")\n",
    "    ax2.set_title(\"Position $y_k$\")\n",
    "    ax3.set_title(\"Object\")\n",
    "    ax1.set_xticks([], [])\n",
    "    ax2.set_xticks([], [])\n",
    "\n",
    "    # Animate plot over time\n",
    "    for k in range(0, L, 2):\n",
    "        ax1.plot(ks[:k], u[:k], color=\"red\")\n",
    "        ax2.plot(ks[:k], y[:k], color=\"blue\")\n",
    "        ax3.boxplot(\n",
    "            [[y[k, 0] - 0.04, y[k, 0], y[k, 0] + 0.04]],\n",
    "            showcaps=False,\n",
    "            whis=False,\n",
    "            vert=False,\n",
    "            widths=10,\n",
    "        )\n",
    "        camera.snap()\n",
    "    anim = camera.animate()\n",
    "    anim.save(\"line.gif\", dpi=150, writer=\"imagemagick\")\n",
    "if False:\n",
    "    example_ssm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_ssm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mexample_ssm\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'example_ssm' is not defined"
     ]
    }
   ],
   "source": [
    "example_ssm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
